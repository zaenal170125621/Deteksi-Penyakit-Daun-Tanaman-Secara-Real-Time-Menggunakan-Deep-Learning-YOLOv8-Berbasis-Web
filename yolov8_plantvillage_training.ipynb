{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2097caf",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ae785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ultralytics\n",
    "!pip install -q scikit-learn\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a722a",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Download Dataset from KaggleHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074aa43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PlantVillage dataset for YOLO object detection\n",
    "# This will download and cache the dataset locally\n",
    "path = kagglehub.dataset_download(\"sebastianpalaciob/plantvillage-for-object-detection-yolo\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"\\nüìÅ Dataset downloaded successfully!\")\n",
    "\n",
    "# Store the dataset root path for later use\n",
    "dataset_root = Path(path)\n",
    "print(f\"Dataset root: {dataset_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f867e2a",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify Dataset Structure and Locate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and verify the actual dataset structure\n",
    "print(\"üîç Exploring dataset structure...\\n\")\n",
    "\n",
    "# List top-level contents\n",
    "print(\"üìÅ Top-level contents:\")\n",
    "for item in dataset_root.iterdir():\n",
    "    item_type = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
    "    print(f\"{item_type} {item.name}\")\n",
    "\n",
    "# Check for common structures\n",
    "print(\"\\nüîç Checking for dataset directories...\")\n",
    "\n",
    "# Check if Dataset folder exists\n",
    "if (dataset_root / \"Dataset\").exists():\n",
    "    print(\"‚úÖ Found 'Dataset' folder\")\n",
    "    dataset_dir = dataset_root / \"Dataset\"\n",
    "    \n",
    "    # List Dataset contents\n",
    "    print(\"\\nüìÅ Contents of Dataset folder:\")\n",
    "    for item in dataset_dir.iterdir():\n",
    "        item_type = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
    "        print(f\"{item_type} {item.name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'Dataset' folder not found, using root as dataset directory\")\n",
    "    dataset_dir = dataset_root\n",
    "\n",
    "# Check for images and labels\n",
    "images_dir = dataset_dir / \"images\"\n",
    "labels_dir = dataset_dir / \"labels\"\n",
    "\n",
    "if images_dir.exists() and labels_dir.exists():\n",
    "    num_images = len(list(images_dir.glob('*.*')))\n",
    "    num_labels = len(list(labels_dir.glob('*.txt')))\n",
    "    print(f\"\\n‚úÖ Found images: {num_images}\")\n",
    "    print(f\"‚úÖ Found labels: {num_labels}\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå images or labels folder not found!\")\n",
    "\n",
    "# Check for classes file\n",
    "classes_file = None\n",
    "for possible_path in [dataset_root / \"classes.yaml\", dataset_dir / \"classes.yaml\"]:\n",
    "    if possible_path.exists():\n",
    "        print(f\"\\n‚úÖ Found classes.yaml at: {possible_path}\")\n",
    "        classes_file = possible_path\n",
    "        break\n",
    "\n",
    "if classes_file is None:\n",
    "    raise ValueError(\"‚ùå classes.yaml not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Dataset structure verified!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3a359",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Read Classes and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read classes from classes.yaml\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "print(\"üìñ Reading classes.yaml...\\n\")\n",
    "\n",
    "# Read classes\n",
    "with open(classes_file, 'r') as f:\n",
    "    classes_data = yaml.safe_load(f)\n",
    "\n",
    "print(\"Classes data:\")\n",
    "print(classes_data)\n",
    "\n",
    "# Extract class names\n",
    "if isinstance(classes_data, dict) and 'names' in classes_data:\n",
    "    class_names = classes_data['names']\n",
    "elif isinstance(classes_data, list):\n",
    "    class_names = classes_data\n",
    "else:\n",
    "    # Assume it's a simple list or dict\n",
    "    class_names = list(classes_data.values()) if isinstance(classes_data, dict) else classes_data\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nüìä Total classes: {num_classes}\")\n",
    "print(f\"First 5 classes: {class_names[:5]}\")\n",
    "\n",
    "# Get all image files\n",
    "image_files = sorted(\n",
    "    list(images_dir.glob('*.jpg')) + \n",
    "    list(images_dir.glob('*.png')) + \n",
    "    list(images_dir.glob('*.jpeg'))\n",
    ")\n",
    "print(f\"\\nüì∏ Total images found: {len(image_files)}\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    raise ValueError(\"‚ùå No image files found! Check the dataset structure.\")\n",
    "\n",
    "# Create train/val/test directories\n",
    "print(\"\\nüìÅ Creating train/val/test directory structure...\")\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    (dataset_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (dataset_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directories created\")\n",
    "\n",
    "# Split dataset: 70% train, 20% valid, 10% test\n",
    "random.seed(42)\n",
    "image_names = [img.name for img in image_files]\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "train_names, temp_names = train_test_split(image_names, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: 20% valid, 10% test (from the 30% temp)\n",
    "valid_names, test_names = train_test_split(temp_names, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  Train: {len(train_names)} images ({len(train_names)/len(image_names)*100:.1f}%)\")\n",
    "print(f\"  Valid: {len(valid_names)} images ({len(valid_names)/len(image_names)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_names)} images ({len(test_names)/len(image_names)*100:.1f}%)\")\n",
    "\n",
    "# Copy files to respective directories\n",
    "print(\"\\nüì¶ Copying files to train/valid/test folders...\")\n",
    "print(\"‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "def copy_files(file_names, split):\n",
    "    copied = 0\n",
    "    for name in file_names:\n",
    "        # Copy image\n",
    "        src_img = images_dir / name\n",
    "        dst_img = dataset_dir / split / 'images' / name\n",
    "        if src_img.exists():\n",
    "            shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "            # Copy corresponding label\n",
    "            label_name = src_img.stem + '.txt'\n",
    "            src_lbl = labels_dir / label_name\n",
    "            dst_lbl = dataset_dir / split / 'labels' / label_name\n",
    "            if src_lbl.exists():\n",
    "                shutil.copy2(src_lbl, dst_lbl)\n",
    "                copied += 1\n",
    "    return copied\n",
    "\n",
    "train_copied = copy_files(train_names, 'train')\n",
    "valid_copied = copy_files(valid_names, 'valid')\n",
    "test_copied = copy_files(test_names, 'test')\n",
    "\n",
    "print(f\"  ‚úÖ Train: {train_copied} image-label pairs copied\")\n",
    "print(f\"  ‚úÖ Valid: {valid_copied} image-label pairs copied\")\n",
    "print(f\"  ‚úÖ Test:  {test_copied} image-label pairs copied\")\n",
    "print(\"\\n‚úÖ Dataset split complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa58c82",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create data.yaml Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml configuration file for YOLOv8\n",
    "print(\"üìù Creating data.yaml configuration...\\n\")\n",
    "\n",
    "# Create data.yaml content\n",
    "data_config = {\n",
    "    'path': str(dataset_dir.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': num_classes,\n",
    "    'names': class_names\n",
    "}\n",
    "\n",
    "print(\"data.yaml configuration:\")\n",
    "print(yaml.dump(data_config, default_flow_style=False, sort_keys=False))\n",
    "\n",
    "# Write data.yaml file\n",
    "yaml_path = dataset_dir / \"data.yaml\"\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"\\n‚úÖ data.yaml created at: {yaml_path}\")\n",
    "print(f\"\\nüìä Dataset configuration:\")\n",
    "print(f\"  Classes: {num_classes}\")\n",
    "print(f\"  Path: {data_config['path']}\")\n",
    "print(f\"  Train: {data_config['train']}\")\n",
    "print(f\"  Val: {data_config['val']}\")\n",
    "print(f\"  Test: {data_config['test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44b87b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability using nvidia-smi\n",
    "print(\"üñ•Ô∏è Checking GPU availability...\\n\")\n",
    "!nvidia-smi\n",
    "\n",
    "# Verify PyTorch can access GPU\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: No GPU detected. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b41e70",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Train YOLOv8 with OOM Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLOv8 with automatic batch size reduction on OOM\n",
    "from ultralytics import YOLO\n",
    "import gc\n",
    "\n",
    "# Training parameters\n",
    "model_name = 'yolov8n.pt'  # Lightweight YOLOv8 nano model\n",
    "img_size = 640\n",
    "epochs = 50\n",
    "batch_sizes = [16, 8, 4]  # Try these batch sizes in order if OOM occurs\n",
    "\n",
    "print(f\"üöÄ Starting YOLOv8 training...\\n\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Image size: {img_size}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Dataset: {yaml_path}\\n\")\n",
    "\n",
    "# Try training with different batch sizes if OOM occurs\n",
    "trained = False\n",
    "for batch_size in batch_sizes:\n",
    "    try:\n",
    "        print(f\"\\nüì¶ Attempting training with batch size: {batch_size}\")\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Initialize model\n",
    "        model = YOLO(model_name)\n",
    "        \n",
    "        # Train the model\n",
    "        results = model.train(\n",
    "            data=str(yaml_path),\n",
    "            epochs=epochs,\n",
    "            imgsz=img_size,\n",
    "            batch=batch_size,\n",
    "            patience=10,  # Early stopping patience\n",
    "            save=True,\n",
    "            project='runs/detect',\n",
    "            name='train',\n",
    "            exist_ok=True,\n",
    "            pretrained=True,\n",
    "            optimizer='auto',\n",
    "            verbose=True,\n",
    "            seed=42,\n",
    "            deterministic=False,\n",
    "            single_cls=False,\n",
    "            rect=False,\n",
    "            cos_lr=False,\n",
    "            close_mosaic=10,\n",
    "            resume=False,\n",
    "            amp=True,  # Automatic Mixed Precision\n",
    "            fraction=1.0,\n",
    "            profile=False,\n",
    "            overlap_mask=True,\n",
    "            mask_ratio=4,\n",
    "            dropout=0.0,\n",
    "            val=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed successfully with batch size {batch_size}!\")\n",
    "        trained = True\n",
    "        break\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"\\n‚ö†Ô∏è OOM Error with batch size {batch_size}\")\n",
    "            if batch_size == batch_sizes[-1]:\n",
    "                print(\"\\n‚ùå Failed with smallest batch size. Cannot continue.\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"Retrying with smaller batch size...\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Training error: {e}\")\n",
    "            raise\n",
    "\n",
    "if not trained:\n",
    "    print(\"\\n‚ùå Training failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fdf2b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Display Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17357491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results and plots\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "print(\"üìä Training Results\\n\")\n",
    "\n",
    "# Find the training results directory\n",
    "results_dir = Path('runs/detect/train')\n",
    "\n",
    "if results_dir.exists():\n",
    "    # Display results plots\n",
    "    plot_files = ['results.png', 'confusion_matrix.png', 'F1_curve.png', 'PR_curve.png', 'P_curve.png', 'R_curve.png']\n",
    "    \n",
    "    for plot_file in plot_files:\n",
    "        plot_path = results_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\nüìà {plot_file}\")\n",
    "            display(Image(filename=str(plot_path)))\n",
    "    \n",
    "    # Display sample predictions\n",
    "    val_batch_files = list(results_dir.glob('val_batch*.jpg'))\n",
    "    if val_batch_files:\n",
    "        print(f\"\\nüñºÔ∏è Sample validation predictions:\")\n",
    "        for img_file in val_batch_files[:3]:  # Show first 3 batches\n",
    "            print(f\"\\n{img_file.name}\")\n",
    "            display(Image(filename=str(img_file)))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Results directory not found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae842c",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Run Validation and Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c599958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model and run validation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "best_model_path = 'runs/detect/train/weights/best.pt'\n",
    "\n",
    "if Path(best_model_path).exists():\n",
    "    print(\"üîç Running validation on best model...\\n\")\n",
    "    \n",
    "    # Load best model\n",
    "    model = YOLO(best_model_path)\n",
    "    \n",
    "    # Run validation\n",
    "    metrics = model.val(data=str(yaml_path), split='val')\n",
    "    \n",
    "    # Display key metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä VALIDATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"mAP50-95:  {metrics.box.map:.4f}\")\n",
    "    print(f\"mAP50:     {metrics.box.map50:.4f}\")\n",
    "    print(f\"mAP75:     {metrics.box.map75:.4f}\")\n",
    "    print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "    print(f\"Recall:    {metrics.box.mr:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display per-class metrics if available\n",
    "    if hasattr(metrics.box, 'maps'):\n",
    "        print(\"\\nüìã Per-class mAP50 (first 10 classes):\")\n",
    "        for i, map_value in enumerate(metrics.box.maps[:10]):\n",
    "            class_name = class_names[i] if i < len(class_names) else f\"Class {i}\"\n",
    "            print(f\"  {class_name}: {map_value:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ùå Best model not found at {best_model_path}\")\n",
    "    print(\"Training may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a5005",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Run Inference on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on test images with low confidence threshold\n",
    "import random\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if Path(best_model_path).exists():\n",
    "    print(\"üéØ Running inference on test images...\\n\")\n",
    "    \n",
    "    # Load best model\n",
    "    model = YOLO(best_model_path)\n",
    "    \n",
    "    # Get test images\n",
    "    test_images_dir = dataset_dir / 'test' / 'images'\n",
    "    test_images = list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png'))\n",
    "    \n",
    "    if test_images:\n",
    "        # Select random test images\n",
    "        num_samples = min(5, len(test_images))\n",
    "        sample_images = random.sample(test_images, num_samples)\n",
    "        \n",
    "        print(f\"Selected {num_samples} random test images for prediction\\n\")\n",
    "        \n",
    "        # Run predictions\n",
    "        results = model.predict(\n",
    "            source=sample_images,\n",
    "            conf=0.10,  # Low confidence threshold to catch all detections\n",
    "            iou=0.45,\n",
    "            save=True,\n",
    "            project='runs/detect',\n",
    "            name='test_predictions',\n",
    "            exist_ok=True,\n",
    "            save_txt=True,\n",
    "            save_conf=True,\n",
    "            show_labels=True,\n",
    "            show_conf=True,\n",
    "            line_width=2,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Predictions completed!\\n\")\n",
    "        \n",
    "        # Display prediction results\n",
    "        pred_dir = Path('runs/detect/test_predictions')\n",
    "        if pred_dir.exists():\n",
    "            pred_images = list(pred_dir.glob('*.jpg')) + list(pred_dir.glob('*.png'))\n",
    "            print(f\"üñºÔ∏è Displaying prediction results:\\n\")\n",
    "            for pred_img in pred_images[:5]:\n",
    "                print(f\"\\n{pred_img.name}\")\n",
    "                display(Image(filename=str(pred_img), width=800))\n",
    "        \n",
    "        # Print detection statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä DETECTION STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        for i, result in enumerate(results):\n",
    "            num_detections = len(result.boxes)\n",
    "            img_name = sample_images[i].name\n",
    "            print(f\"{img_name}: {num_detections} detections\")\n",
    "            \n",
    "            if num_detections > 0:\n",
    "                for box in result.boxes:\n",
    "                    cls = int(box.cls[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    class_name = class_names[cls] if cls < len(class_names) else f\"Class {cls}\"\n",
    "                    print(f\"  - {class_name}: {conf:.3f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No test images found in {test_images_dir}\")\n",
    "else:\n",
    "    print(f\"‚ùå Best model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449f22e",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Download Trained Weights and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a downloadable archive of training results\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì¶ Preparing files for download...\\n\")\n",
    "\n",
    "# Files to download\n",
    "download_files = [\n",
    "    ('runs/detect/train/weights/best.pt', 'Best model weights'),\n",
    "    ('runs/detect/train/weights/last.pt', 'Last epoch weights'),\n",
    "    ('runs/detect/train/results.png', 'Training results plot'),\n",
    "    ('runs/detect/train/results.csv', 'Training results CSV'),\n",
    "    ('runs/detect/train/confusion_matrix.png', 'Confusion matrix'),\n",
    "]\n",
    "\n",
    "downloaded_count = 0\n",
    "for file_path, description in download_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚¨áÔ∏è Downloading: {description} ({file_path})\")\n",
    "        try:\n",
    "            files.download(file_path)\n",
    "            downloaded_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Failed to download: {e}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è File not found: {file_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded {downloaded_count}/{len(download_files)} files\")\n",
    "\n",
    "# Optionally create a zip archive\n",
    "print(\"\\nüì¶ Creating zip archive of all results...\")\n",
    "if Path('runs/detect/train').exists():\n",
    "    shutil.make_archive('yolov8_training_results', 'zip', 'runs/detect/train')\n",
    "    print(\"‚¨áÔ∏è Downloading complete training results archive...\")\n",
    "    files.download('yolov8_training_results.zip')\n",
    "    print(\"‚úÖ Archive downloaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training results directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6da179",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Troubleshooting Notes\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. Empty Detections\n",
    "**Symptoms:** Model doesn't detect anything or very few detections\n",
    "\n",
    "**Solutions:**\n",
    "- **Lower confidence threshold:** Change `conf=0.10` to `conf=0.05` or even `conf=0.01`\n",
    "- **Check label format:** YOLO labels must be normalized (0..1 range)\n",
    "  ```python\n",
    "  # Verify label format\n",
    "  with open('path/to/label.txt', 'r') as f:\n",
    "      for line in f:\n",
    "          values = line.strip().split()\n",
    "          # Format: class_id x_center y_center width height\n",
    "          # All coordinates should be between 0 and 1\n",
    "          print(values)\n",
    "  ```\n",
    "- **Verify image-label pairs:** Ensure each image has a corresponding label file\n",
    "- **Check class indices:** Class IDs in labels must match the names list in data.yaml (0-indexed)\n",
    "\n",
    "#### 2. Training Fails or Poor Performance\n",
    "**Solutions:**\n",
    "- **Insufficient training data:** Ensure you have enough samples per class (minimum 50-100)\n",
    "- **Increase epochs:** Try 100-200 epochs for better convergence\n",
    "- **Adjust learning rate:** Use `lr0=0.01` for initial learning rate\n",
    "- **Data augmentation:** Ultralytics applies augmentation by default\n",
    "\n",
    "#### 3. OOM (Out of Memory) Errors\n",
    "**Solutions:**\n",
    "- **Reduce batch size:** The code already handles this automatically (16‚Üí8‚Üí4)\n",
    "- **Reduce image size:** Change `imgsz=640` to `imgsz=416` or `imgsz=320`\n",
    "- **Use smaller model:** Keep using `yolov8n.pt` (nano is smallest)\n",
    "\n",
    "#### 4. Dataset Format Issues\n",
    "**Solutions:**\n",
    "- **Verify YOLO format:** Each label file should contain:\n",
    "  ```\n",
    "  class_id x_center y_center width height\n",
    "  ```\n",
    "  All values normalized to 0-1 range\n",
    "- **Matching filenames:** image.jpg should have corresponding label image.txt\n",
    "\n",
    "#### 5. Low mAP Scores\n",
    "**Solutions:**\n",
    "- **Increase training time:** More epochs often improve mAP\n",
    "- **Use larger model:** Upgrade from nano (n) to small (s): `yolov8s.pt`\n",
    "- **Review confusion matrix:** Identify which classes are confused\n",
    "\n",
    "### Additional Tips:\n",
    "- **Resume training:** If training is interrupted:\n",
    "  ```python\n",
    "  model = YOLO('runs/detect/train/weights/last.pt')\n",
    "  model.train(resume=True)\n",
    "  ```\n",
    "- **Export model:** Convert to other formats:\n",
    "  ```python\n",
    "  model.export(format='onnx')  # or 'tflite', 'coreml', etc.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b081eed",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Diagnostic Cell\n",
    "Run this cell if you encounter issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic checks\n",
    "print(\"üîß Running diagnostics...\\n\")\n",
    "\n",
    "# 1. Check dataset structure\n",
    "print(\"1Ô∏è‚É£ Dataset Structure:\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    img_dir = dataset_dir / split / 'images'\n",
    "    lbl_dir = dataset_dir / split / 'labels'\n",
    "    if img_dir.exists() and lbl_dir.exists():\n",
    "        num_images = len(list(img_dir.glob('*.*')))\n",
    "        num_labels = len(list(lbl_dir.glob('*.txt')))\n",
    "        print(f\"  {split}: {num_images} images, {num_labels} labels\")\n",
    "    else:\n",
    "        print(f\"  {split}: ‚ö†Ô∏è Missing directories\")\n",
    "\n",
    "# 2. Check a sample label file\n",
    "print(\"\\n2Ô∏è‚É£ Sample Label Format:\")\n",
    "label_files = list((dataset_dir / 'train' / 'labels').glob('*.txt'))\n",
    "if label_files:\n",
    "    with open(label_files[0], 'r') as f:\n",
    "        lines = f.readlines()[:3]\n",
    "        for line in lines:\n",
    "            values = line.strip().split()\n",
    "            if len(values) >= 5:\n",
    "                cls, x, y, w, h = values[:5]\n",
    "                in_range = all(0 <= float(v) <= 1 for v in [x, y, w, h])\n",
    "                status = \"‚úÖ\" if in_range else \"‚ùå\"\n",
    "                print(f\"  {status} class={cls}, x={x}, y={y}, w={w}, h={h}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No label files found\")\n",
    "\n",
    "# 3. Check data.yaml\n",
    "print(\"\\n3Ô∏è‚É£ data.yaml Configuration:\")\n",
    "if yaml_path.exists():\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        print(f\"  Path: {config.get('path', 'NOT SET')}\")\n",
    "        print(f\"  Classes: {config.get('nc', len(config.get('names', [])))}\")\n",
    "        print(f\"  Train: {config.get('train', 'NOT SET')}\")\n",
    "        print(f\"  Val: {config.get('val', 'NOT SET')}\")\n",
    "else:\n",
    "    print(\"  ‚ùå data.yaml not found\")\n",
    "\n",
    "# 4. Check GPU\n",
    "print(\"\\n4Ô∏è‚É£ GPU Status:\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No GPU available\")\n",
    "\n",
    "print(\"\\n‚úÖ Diagnostics complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
